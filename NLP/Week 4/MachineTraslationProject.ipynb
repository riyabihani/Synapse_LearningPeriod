{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FoXMLNg8PAcO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "from string import digits\n",
        "import re\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import LSTM, Input, Dense,Embedding, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model,load_model, model_from_json\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import pickle as pkl\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('hin.txt','r') as f:\n",
        "  data = f.read()"
      ],
      "metadata": {
        "id": "EWlUQEwhSr6E"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uncleaned_data_list = data.split('\\n')\n",
        "len(uncleaned_data_list)\n",
        "uncleaned_data_list = uncleaned_data_list[:38695]\n",
        "len(uncleaned_data_list)\n",
        "\n",
        "english_word = []\n",
        "hindi_word = []\n",
        "cleaned_data_list = []\n",
        "for word in uncleaned_data_list:\n",
        "  english_word.append(word.split('\\t')[:1][0])\n",
        "  hindi_word.append(word.split('\\t')[:2][-1])\n",
        "  \n",
        "language_data = pd.DataFrame(columns=['English','Hindi'])\n",
        "language_data['English'] = english_word\n",
        "language_data['Hindi'] = hindi_word\n",
        "language_data.to_csv('language_data.csv', index=False)\n",
        "language_data.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "XqTj9m4YSutd",
        "outputId": "347b825a-eef7-4c99-e2bf-6c2ac343cf26"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Hindi</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow!</td>\n",
              "      <td>वाह!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Duck!</td>\n",
              "      <td>झुको!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Duck!</td>\n",
              "      <td>बतख़!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Help!</td>\n",
              "      <td>बचाओ!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Jump.</td>\n",
              "      <td>उछलो.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Jump.</td>\n",
              "      <td>कूदो.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Jump.</td>\n",
              "      <td>छलांग.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Hello!</td>\n",
              "      <td>नमस्ते।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Hello!</td>\n",
              "      <td>नमस्कार।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Cheers!</td>\n",
              "      <td>वाह-वाह!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   English     Hindi\n",
              "0     Wow!      वाह!\n",
              "1    Duck!     झुको!\n",
              "2    Duck!     बतख़!\n",
              "3    Help!     बचाओ!\n",
              "4    Jump.     उछलो.\n",
              "5    Jump.     कूदो.\n",
              "6    Jump.    छलांग.\n",
              "7   Hello!   नमस्ते।\n",
              "8   Hello!  नमस्कार।\n",
              "9  Cheers!  वाह-वाह!"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_text = language_data['English'].values\n",
        "hindi_text = language_data['Hindi'].values\n",
        "len(english_text), len(hindi_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7BefP98T2m3",
        "outputId": "93f632e5-ad1a-49bc-f671-27e25e517d73"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2953, 2953)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to lower case\n",
        "english_text_ = [x.lower() for x in english_text]\n",
        "hindi_text_ = [x.lower() for x in hindi_text]\n",
        "\n",
        "#removing inverted commas\n",
        "english_text_ = [re.sub(\"'\",'',x) for x in english_text_]\n",
        "hindi_text_ = [re.sub(\"'\",'',x) for x in hindi_text_]\n",
        "def remove_punc(text_list):\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  removed_punc_text = []\n",
        "  for sent in text_list:\n",
        "    sentance = [w.translate(table) for w in sent.split(' ')]\n",
        "    removed_punc_text.append(' '.join(sentance))\n",
        "  return removed_punc_text\n",
        "english_text_ = remove_punc(english_text_)\n",
        "hindi_text_ = remove_punc(hindi_text_)\n",
        "remove_digits = str.maketrans('', '', digits)\n",
        "removed_digits_text = []\n",
        "for sent in english_text_:\n",
        "  sentance = [w.translate(remove_digits) for w in sent.split(' ')]\n",
        "  removed_digits_text.append(' '.join(sentance))\n",
        "english_text_ = removed_digits_text\n",
        "\n",
        "# removing the digits from the hindi sentances\n",
        "hindi_text_ = [re.sub(\"[२३०८१५७९४६]\",\"\",x) for x in hindi_text_]\n",
        "hindi_text_ = [re.sub(\"[\\u200d]\",\"\",x) for x in hindi_text_]\n",
        "\n",
        "# removing the stating and ending whitespaces\n",
        "english_text_ = [x.strip() for x in english_text_]\n",
        "hindi_text_ = [x.strip() for x in hindi_text_]"
      ],
      "metadata": {
        "id": "eD27G6jQUOvH"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Putting the start and end words in the hindi sentances\n",
        "hindi_text_ = [\"start \" + x + \" end\" for x in hindi_text_]\n",
        "\n",
        "# manipulated_hindi_text_\n",
        "hindi_text_[0], english_text_[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_taFKaHIU6nt",
        "outputId": "2c542e1c-5637-4d31-9a62-0db6e0f9968c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('start start वाह end end', 'wow')"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = english_text_\n",
        "Y = hindi_text_\n",
        "X_train, X_test, y_train, y_test=train_test_split(X,Y,test_size=0.1)"
      ],
      "metadata": {
        "id": "vhdGJMFJVIiI"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Max_length(data):\n",
        "  max_length_ = max([len(x.split(' ')) for x in data])\n",
        "  return max_length_\n",
        "\n",
        "#Training data\n",
        "max_length_english = Max_length(X_train)\n",
        "max_length_hindi = Max_length(y_train)\n",
        "\n",
        "#Test data\n",
        "max_length_english_test = Max_length(X_test)\n",
        "max_length_hindi_test = Max_length(y_test)\n",
        "max_length_hindi, max_length_english"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHL2rArcVLmr",
        "outputId": "48f70315-3572-42e2-a807-56b2f8ddc086"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(29, 22)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "englishTokenizer = Tokenizer()\n",
        "englishTokenizer.fit_on_texts(X_train)\n",
        "Eword2index = englishTokenizer.word_index\n",
        "vocab_size_source = len(Eword2index) + 1\n",
        "\n",
        "X_train = englishTokenizer.texts_to_sequences(X_train)\n",
        "X_train = pad_sequences(X_train, maxlen=max_length_english, padding='post')\n",
        "\n",
        "X_test = englishTokenizer.texts_to_sequences(X_test)\n",
        "X_test = pad_sequences(X_test, maxlen = max_length_english, padding='post')\n",
        "\n",
        "hindiTokenizer = Tokenizer()\n",
        "hindiTokenizer.fit_on_texts(y_train)\n",
        "Hword2index = hindiTokenizer.word_index\n",
        "vocab_size_target = len(Hword2index) + 1\n",
        "\n",
        "y_train = hindiTokenizer.texts_to_sequences(y_train)\n",
        "y_train = pad_sequences(y_train, maxlen=max_length_hindi, padding='post')\n",
        "\n",
        "y_test = hindiTokenizer.texts_to_sequences(y_test)\n",
        "y_test = pad_sequences(y_test, maxlen = max_length_hindi, padding='post')\n",
        "\n",
        "vocab_size_source, vocab_size_target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b4HRSVlVLqm",
        "outputId": "54846677-f5a5-4aaa-e578-512e5b6bbfb2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2275, 2888)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0], y_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-wSjjoaVLyt",
        "outputId": "905ccb13-ca38-4fe5-fef0-7a7d5a78e122"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 47, 116, 424, 112,   4, 117,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32),\n",
              " array([  1,   1, 363,  22, 327,  69,  16, 953, 328,   5,   3,   2,   2,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0], dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('NMT_data.pkl','wb') as f:\n",
        "  pkl.dump([X_train, y_train, X_test, y_test],f)\n",
        "\n",
        "with open('NMT_Etokenizer.pkl','wb') as f:\n",
        "  pkl.dump([vocab_size_source, Eword2index, englishTokenizer], f)\n",
        "\n",
        "with open('NMT_Htokenizer.pkl', 'wb') as f:\n",
        "  pkl.dump([vocab_size_target, Hword2index, hindiTokenizer], f)\n",
        "  \n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)"
      ],
      "metadata": {
        "id": "-sjrW0PFZPvw"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from attention import AttentionLayer\n",
        "from keras import backend as K \n",
        "K.clear_session() \n",
        "latent_dim = 500\n",
        "\n",
        "# Encoder \n",
        "encoder_inputs = Input(shape=(max_length_english,)) \n",
        "enc_emb = Embedding(vocab_size_source, latent_dim,trainable=True)(encoder_inputs)\n",
        "\n",
        "#LSTM 1 \n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "#LSTM 2 \n",
        "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "#LSTM 3 \n",
        "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) \n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
        "\n",
        "# Set up the decoder. \n",
        "decoder_inputs = Input(shape=(None,)) \n",
        "dec_emb_layer = Embedding(vocab_size_target, latent_dim,trainable=True) \n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "#LSTM using encoder_states as initial state\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
        "\n",
        "#Attention Layer\n",
        "attn_layer = AttentionLayer(name='attention_layer') \n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
        "\n",
        "# Concat attention output and decoder LSTM output \n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "#Dense layer\n",
        "decoder_dense = TimeDistributed(Dense(vocab_size_target, activation='softmax')) \n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
        "plot_model(model, to_file='train_model.png', show_shapes=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "ju80qTc8ZbhG",
        "outputId": "da7f8dc7-4dde-473c-d409-d8a282371652"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-8daf1c687ee2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAttentionLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlatent_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'attention'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eHzrGJInZbox"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}